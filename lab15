# Лабораторная работа №15:
# Машины опорных векторов (SVM).
install.packages("e1071")
install.packages("ROCR")
library(e1071)
library(ROCR)

# библиотека e1071 позволяет реализовать набор методов статистического обучения
# для построения классификатора на опорных векторах можно воспользоваться svm()
# аргумент kernel = "linear" строит линейную границу между классами,
# аргумент cost  позволяет задать штраф за нарушение границ зазора.

# При малом значение аргумента cost зазор будет широким и многие опорные вектора
# будут лежать на границах зазора или выходить из них

# При большом значении аргумента cost  зазор будет узким и лишь немногие опорные 
# векторы будут лежать на его границах или выходить за них

set.seed(1)
x = matrix(rnorm(20*2), ncol = 2)
y = c(rep(-1, 10), rep(1, 10))
x[y == 1,] = x[y == 1, ] + 1

# проверим, являются ли классы линейно-разделимыми
plot(x, col = (3 - y))

# как видно из диаграммы, точки данных разных классов визуально линейно не разделимы

# построим SVM классификатор. Обратите внимание, для классификатора нам понадобится
# закодировать отклик в виде факторной переменной

iris<-read.csv("iris.csv", header = T, sep=",")
for(i in 1:100){ 
  iris[i,5]<-1
}
for(i in 101:150){
  
  iris[i,5]<-2
}
iris <- lapply(iris, as.numeric)
x <- data.frame(iris$sepal.length, iris$sepal.width, 
                iris$petal.length, iris$petal.width)
y <- iris$variety
set.seed(1233)
plot(x, col = (71 - y))

dat = data.frame(x = x, y = as.factor(y))
svm.fit = svm(y ~., data = dat, kernel = "linear",
              cost = 10, scale = F) 
plot(svm.fit, dat,x.iris.petal.width ~ x.iris.petal.length,
     slice = list(x.iris.sepal.width = 3, x.iris.sepal.length = 4))

svmfit$
  summary(svmfit)
set.seed(1)
tune.out = tune(svm, y~., data = dat, kernel = "linear",
                ranges = list(cost = c(0.001, 0.1, 1, 5, 10, 100)))
summary(tune.out)
bestmod = tune.out$best.model
summary(bestmod)
set.seed(1)
train = sample(150, 75)
ypred = predict(bestmod, dat[-train,])
table(predict = ypred, truth = dat[-train,"y"]) 
rocplot = function(pred, truth, ...){
  predob = prediction(pred, truth)
  perf = performance(predob, "tpr", "fpr")
  plot(perf, ...)
}
svmfit.opt = svm(y ~., data = dat[train,], kernel = "radial",
                 gamma = 2, cost = 1, decision.values = T)
fitted = attributes(predict(svmfit.opt, dat[train,], 
                            decision.values = T))$decision.values
par(mfrow = c(1,2))
rocplot(fitted, dat[train, "y"], main = "Training Data")
svmfit.flex = svm(y ~., data = dat[train,], kernel = "radial",
                  gamma = 50, cost = 1, decision.values = T)
fitted = attributes(predict(svmfit.flex, dat[train,], 
                            decision.values = T))$decision.values
rocplot(fitted, dat[train, "y"], main = "Training Data", 
        add = T, col = "red")
fitted = attributes(predict(svmfit.opt, dat[-train,], 
                            decision.values = T))$decision.values
rocplot(fitted, dat[-train,"y"], main = "Test data ")
fitted = attributes(predict(svmfit.flex, dat[-train,], 
                            decision.values = T))$decision.values
rocplot(fitted, dat[-train,"y"], add = T, col = "green")
